{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem: Optimizing Warehouse Robot Movement\n",
        "A warehouse robot needs to pick up an item from a shelf and deliver it to a packing area. The environment is represented as a grid, and the robot must navigate from a starting position to a target position while avoiding penalties for moving inefficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "ngpY8iYXVwH_",
        "outputId": "1bc57401-dcfd-4e47-dd78-df9b492e531c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trained Q-table:\n",
            "[[ 37.35138612  42.612659    42.6126509   37.35138933]\n",
            " [ 42.61265384  37.35133842  48.45851     37.35134595]\n",
            " [ 13.20081665  14.34315964  28.84396433  42.61265426]\n",
            " [  2.44971686   3.53271563  35.17172844   2.52068919]\n",
            " [  3.76636155   4.64220823  18.09355348   0.11582807]\n",
            " [ 29.86279916  48.45851     36.34083768  34.14111746]\n",
            " [ 42.61265827  54.95388681  54.9539      42.61264457]\n",
            " [ 18.3047395   34.12132903  62.17099955  34.63146455]\n",
            " [  5.0631692   11.9304682   61.8646587   18.70860796]\n",
            " [  3.64825356  11.92908537  40.8414766    7.78265956]\n",
            " [ 14.64890403  54.95388898  40.66176262  22.4212957 ]\n",
            " [ 48.45850963  62.17099729  62.171       48.4583122 ]\n",
            " [ 50.59803921  63.04886043  70.19        47.87634376]\n",
            " [ 20.12945934  48.44723634  78.33607169  42.17275512]\n",
            " [ 14.34537934  38.56387777  76.73118132  27.18158006]\n",
            " [ 19.11061782  62.17098883  36.62865837  32.09131997]\n",
            " [ 54.95388814  70.18999397  70.19        54.95366625]\n",
            " [ 58.42769924  77.5773694   79.1         61.31964992]\n",
            " [ 54.42548242  82.30116971  89.          60.84133798]\n",
            " [ 25.97809842  59.44505422  99.21448328  44.73132344]\n",
            " [ 17.18609459  70.18999182  46.6022187   44.91457544]\n",
            " [ 62.17097557  79.1         70.18999823  62.17087347]\n",
            " [ 70.18999997  89.          79.09999983  70.18999987]\n",
            " [ 79.09999969 100.          88.99999945  79.09999994]\n",
            " [  0.           0.           0.           0.        ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Environment: 5x5 grid\n",
        "grid_size = 5\n",
        "states = grid_size * grid_size  # Total states\n",
        "actions = 4  # Actions: 0=Up, 1=Right, 2=Down, 3=Left\n",
        "rewards = -1 * np.ones((grid_size, grid_size))  # Default penalty for every move\n",
        "rewards[4, 4] = 100  # Reward for reaching the target (bottom-right corner)\n",
        "target_state = (4, 4)\n",
        "\n",
        "# Transition logic\n",
        "def next_position(state, action):\n",
        "    row, col = divmod(state, grid_size)\n",
        "    if action == 0 and row > 0:  # Move up\n",
        "        row -= 1\n",
        "    elif action == 1 and col < grid_size - 1:  # Move right\n",
        "        col += 1\n",
        "    elif action == 2 and row < grid_size - 1:  # Move down\n",
        "        row += 1\n",
        "    elif action == 3 and col > 0:  # Move left\n",
        "        col -= 1\n",
        "    return row * grid_size + col\n",
        "\n",
        "# Q-learning Parameters\n",
        "q_table = np.zeros((states, actions))\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_decay = 0.995\n",
        "episodes = 5000\n",
        "\n",
        "# Q-Learning\n",
        "for _ in range(episodes):\n",
        "    state = 0  # Start at top-left corner\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Choose action (epsilon-greedy)\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.choice(actions)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        next_state = next_position(state, action)\n",
        "        reward = rewards[divmod(next_state, grid_size)]\n",
        "        done = next_state == target_state[0] * grid_size + target_state[1]\n",
        "\n",
        "        # Update Q-value\n",
        "        best_next_action = np.max(q_table[next_state])\n",
        "        q_table[state, action] += alpha * (reward + gamma * best_next_action - q_table[state, action])\n",
        "        state = next_state\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(epsilon * epsilon_decay, 0.1)\n",
        "\n",
        "# Display the trained Q-table\n",
        "print(\"Trained Q-table:\")\n",
        "print(q_table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics\n",
        "\n",
        "- **Convergence:** Check if the robot learns to reach the target.\n",
        "- **Path Efficiency:** Calculate the total number of steps taken.\n",
        "- **Reward Accumulation:** Ensure the robot maximizes cumulative rewards."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
