{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem: Predict Optimal Daily Electricity Usage\n",
        "\n",
        "Using RL to determine the optimal daily electricity usage to minimize cost, given varying energy prices and household demand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "ngpY8iYXVwH_",
        "outputId": "1bc57401-dcfd-4e47-dd78-df9b492e531c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal Policy (action for each price level): [2 2 2 2]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the environment\n",
        "prices = [5, 10, 20, 30]  # Energy price per unit (arbitrary scale)\n",
        "demand = [2, 4, 6, 8]  # Energy demand in units (arbitrary scale)\n",
        "actions = [0, 1, 2]  # Actions: 0=Low usage, 1=Medium usage, 2=High usage\n",
        "states = len(prices)\n",
        "\n",
        "# Define rewards: Higher usage during low price is better, penalize otherwise\n",
        "def get_reward(state, action):\n",
        "    if action == 0:  # Low usage\n",
        "        return 10 - prices[state]\n",
        "    elif action == 1:  # Medium usage\n",
        "        return 20 - prices[state]\n",
        "    elif action == 2:  # High usage\n",
        "        return 30 - prices[state]\n",
        "    return -100  # Invalid\n",
        "\n",
        "# Q-learning Parameters\n",
        "q_table = np.zeros((states, len(actions)))\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_decay = 0.995\n",
        "episodes = 1000\n",
        "\n",
        "# Q-learning Algorithm\n",
        "for _ in range(episodes):\n",
        "    state = np.random.choice(states)  # Start with a random price level\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Choose action\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.choice(actions)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        # Compute reward and next state\n",
        "        reward = get_reward(state, action)\n",
        "        next_state = (state + 1) % states  # Simulate next price level\n",
        "        done = next_state == 0\n",
        "\n",
        "        # Update Q-value\n",
        "        best_next_action = np.max(q_table[next_state])\n",
        "        q_table[state, action] += alpha * (reward + gamma * best_next_action - q_table[state, action])\n",
        "        state = next_state\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(epsilon * epsilon_decay, 0.1)\n",
        "\n",
        "# Optimal policy\n",
        "optimal_policy = np.argmax(q_table, axis=1)\n",
        "print(f\"Optimal Policy (action for each price level): {optimal_policy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics\n",
        "\n",
        "- **Optimal Policy:** Check if the policy aligns with minimizing costs.\n",
        "- **Convergence:** Ensure the Q-values stabilize.\n",
        "- **Energy Cost:** Calculate the total cost for a simulated day using the learned policy.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
